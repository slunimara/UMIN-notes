{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ukázka Reinforcement Learning v knihovně Gymnasium\n",
    "- open source knihovna pro vývoj algoritmů reinforcement learning\n",
    "- poskytuje API pro komunikaci mezi algoritmy učení a a různými prostředími\n",
    "- jedná se o fork knihovny Gym (která už není spravovaná) od vývojářů z OpenAI\n",
    "- [Odkaz na knihovnu](https://gymnasium.farama.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nainstaluje se do Python prostředí Jupyter notebooku\n",
    "# když odkomentujete následující řádky a spustíte kód, tak se knihovny nainstalují automaticky\n",
    "\n",
    "# %pip install gymnasium\n",
    "# %pip install swig\n",
    "# %pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- můžete si vybrat ze široké škály prostředí.\n",
    "    - prostředí lze také modifikovat různými parametry\n",
    "- v knihovně můžete najít klasických 2D prostředí kde se snažíte, aby formule nenabourala, nebo různě pohybující se 3D kostry člověka, a nebo prostředí, kde lze naučit agenta hrát blackjack\n",
    "- pokud vám, žádné z nich nevyhovuje, tak lze nainportovat externí prostředí (třeba ze hry TrackMania)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== classic_control =====\n",
      "Acrobot-v1             CartPole-v0            CartPole-v1\n",
      "MountainCar-v0         MountainCarContinuous-v0 Pendulum-v1\n",
      "===== phys2d =====\n",
      "phys2d/CartPole-v0     phys2d/CartPole-v1     phys2d/Pendulum-v0\n",
      "===== box2d =====\n",
      "BipedalWalker-v3       BipedalWalkerHardcore-v3 CarRacing-v3\n",
      "LunarLander-v3         LunarLanderContinuous-v3\n",
      "===== toy_text =====\n",
      "Blackjack-v1           CliffWalking-v0        FrozenLake-v1\n",
      "FrozenLake8x8-v1       Taxi-v3\n",
      "===== tabular =====\n",
      "tabular/Blackjack-v0   tabular/CliffWalking-v0\n",
      "===== mujoco =====\n",
      "Ant-v2                 Ant-v3                 Ant-v4\n",
      "Ant-v5                 HalfCheetah-v2         HalfCheetah-v3\n",
      "HalfCheetah-v4         HalfCheetah-v5         Hopper-v2\n",
      "Hopper-v3              Hopper-v4              Hopper-v5\n",
      "Humanoid-v2            Humanoid-v3            Humanoid-v4\n",
      "Humanoid-v5            HumanoidStandup-v2     HumanoidStandup-v4\n",
      "HumanoidStandup-v5     InvertedDoublePendulum-v2 InvertedDoublePendulum-v4\n",
      "InvertedDoublePendulum-v5 InvertedPendulum-v2    InvertedPendulum-v4\n",
      "InvertedPendulum-v5    Pusher-v2              Pusher-v4\n",
      "Pusher-v5              Reacher-v2             Reacher-v4\n",
      "Reacher-v5             Swimmer-v2             Swimmer-v3\n",
      "Swimmer-v4             Swimmer-v5             Walker2d-v2\n",
      "Walker2d-v3            Walker2d-v4            Walker2d-v5\n",
      "===== None =====\n",
      "GymV21Environment-v0   GymV26Environment-v0\n"
     ]
    }
   ],
   "source": [
    "# Výpis všech dostupných prostředí\n",
    "gym.pprint_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Přistání na měsíci\n",
    "- my si vybereme prostředí [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), kde je cílem, aby lunární modul bezpečně přistál na \"měsíci\"\n",
    "- jedná se o problém optimalizace trajektorie rakety\n",
    "- náš modul má diskrétní prostor akcí ze kterých může vybírat\n",
    "    - 0: nedělat nic\n",
    "    - 1: spustí levý balanční motor\n",
    "    - 2: spustí hlavní motor\n",
    "    - 3: spustí pravý balanční motor\n",
    "- **observation** je reprezentován jako osmirozměrný vektor: \n",
    "    - souřadnice modulu v x a y\n",
    "    - jeho lineární rychlosti v x a y\n",
    "    - jeho úhel naklonění a úhlová rychlost\n",
    "    - dva booleany, které vyjadřují, zda je každá noha v kontaktu se zemí, nebo ne\n",
    "- chceme **agenta** naučit bezpečnému přistání modulu\n",
    "    - cílem je získat dostatečný počet bodů (cíl je 200 bodů), které lze získat za různé úkoly v každém kroku\n",
    "    - třeba přistátní modulu je hodnoceno 100 body, naopak -100 body havarování\n",
    "    - ale i rychlost pohybu, či výška modulu je hodnocena\n",
    "- informaci o prostředí je spousta, takže spíš si doporučuji přečíst dokumentaci :)\n",
    "- k vytvoření ukázky jsem využil následující [návod](https://www.geeksforgeeks.org/actor-critic-algorithm-in-reinforcement-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nejprve si ukážeme jak to vůbec funguje\n",
    "# Ukážeme si jeden běh epizody\n",
    "# Epizoda je běh modelu v nějakém prostředí (konec epizody může být například havarie modulu nebo jeho přistání)\n",
    "# Během učení může agent projít mnoha epizodami\n",
    "\n",
    "# Inicializace prostředí přistání na měsíci\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "# Hodnota gym.Env vrácená funkcí gym.make je třída reprezentující Markovův rozhodovací proces\n",
    "# Více: https://gymnasium.farama.org/api/env/#gymnasium.Env\n",
    "\n",
    "# Zahájíme novou epizodu\n",
    "observation, info = env.reset(seed=42)\n",
    "episode_over = False\n",
    "# Iterace časem v dané epizodě\n",
    "while not episode_over:\n",
    "    # Zde v aktuálním případě bereme náhodnou akci z prostoru akcí\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Funkce step provede vybranou akci v prostředí\n",
    "    # výsledkem je observation, reward a informaci o tom zda epizoda byla terminated (například v případě havárie) nebo truncaded (když chceme pevný počet časových kroků)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Pokud nějakým způsobem epizoda skončila, tak zahájíme novou (restartujeme prostředí)\n",
    "    if terminated or truncated:\n",
    "        episode_over = True\n",
    "\n",
    "env.close()\n",
    "# Pozn. Pokud se pokusíte zavřít okno, tak se vám může stát, že vám spadne celý Jupyter kernel.\n",
    "# Doporučuji nehcávat okno otevřené na pozadí. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Keras version: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic algoritmus\n",
    "- kombinuje policy-based a value-based přístupy\n",
    "- skládá se ze dvou konceptů\n",
    "    - Actor (herec) – reprezentuje politiku, tedy stárá se o to, jak agent jedná\n",
    "    - Critic (kritik) – poskytuje zpětnou vazbu herci tím, že hodnotí jeho akce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# Síť nám bude předkládat akce, kde akce s největší pravděpodobností by měla maximalizovat šanci na úspěch\n",
    "actor = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(env.action_space.n, activation='softmax')\n",
    "])\n",
    "\n",
    "critic = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "actor_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "critic_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 100\n",
    "gamma = 0.99\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    observation, info = env.reset(seed=42)\n",
    "    episode_reward = 0\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Omezení počtu iterací v epizodě\n",
    "        for t in range(1, 10000):  \n",
    "            # Vypočítáme pravděpodobnost akcí\n",
    "            action_probs = actor(np.array([observation]))\n",
    "            # Výpočet očekávané hodnoty odměny\n",
    "            state_value = critic(np.array([observation]))[0, 0]\n",
    "            # Výběr z akcí náhodně, ale s přihlédnutím na vypočítanou pravděpodobnost akcí\n",
    "            action = np.random.choice(env.action_space.n, p=action_probs.numpy()[0])\n",
    "\n",
    "            # Provedeme vybranou akci\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Výpočet aktuální hodnoty odměny\n",
    "            next_state_value = critic(np.array([observation]))[0, 0]\n",
    "            # Temporal Difference – chyba měří, rozdíl mezi očekávanou a aktuální hodnotou odměny\n",
    "            td = reward + gamma * next_state_value - state_value\n",
    "\n",
    "            # Vypočítáme ztráty\n",
    "            actor_loss = -tf.math.log(action_probs[0, action]) * td\n",
    "            critic_loss = tf.square(td)\n",
    "\n",
    "            # Aktualizujeme váhy modelů na základě ztrát\n",
    "            actor_gradients = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "            critic_gradients = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            actor_optimizer.apply_gradients(zip(actor_gradients, actor.trainable_variables))\n",
    "            critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "\n",
    "            if terminated or truncated or abs(reward) == 100:\n",
    "                break\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Episode {i}, Reward: {episode_reward}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
